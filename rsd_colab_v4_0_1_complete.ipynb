{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Garmin RSD \u2192 Side-Scan \u2192 Google Earth (Colab v4.0.1, Complete)\n", "_Generated: 2025-09-05 18:36_\n", "\n", "**What\u2019s in this notebook**\n", "- Garmin `.RSD` parser (spec-style varuint/zigzag)\n", "- Grayscale cache \u2192 recolor (palettes: amber, blue, grayscale, green, ironbow)\n", "- Tiling with **row-width padding fix** (handles sample count changes mid-recording)\n", "- Three KML modes: **SINGLE**, **BUCKETED**, **REGIONATED** (LOD)\n", "- GPS sanity markers, depth overlay, simple target detection\n", "- Optional waterfall video (ffmpeg)\n", "- ZIP export (optionally includes strips)\n", "- **Synthetic sample `.RSD`** so you can run end-to-end without real data\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": ["# ===============================\n", "# 1) Mount Google Drive (optional)\n", "# ===============================\n", "from google.colab import drive\n", "import os, datetime\n", "TRY_MOUNT_DRIVE = True  # set False to skip mounting\n", "if TRY_MOUNT_DRIVE:\n", "    drive.mount('/content/drive')\n", "    OUTPUT_PARENT_DIR = '/content/drive/MyDrive/rsd_runs'\n", "else:\n", "    OUTPUT_PARENT_DIR = '/content/rsd_runs'\n", "os.makedirs(OUTPUT_PARENT_DIR, exist_ok=True)\n", "\n", "OUTPUT_RUN_NAME = f\"run_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n", "OUT_DIR = os.path.join(OUTPUT_PARENT_DIR, OUTPUT_RUN_NAME)\n", "os.makedirs(OUT_DIR, exist_ok=True)\n", "print('OUT_DIR:', OUT_DIR)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Install dependencies"]}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": ["!pip -q install simplekml tqdm pillow numpy imageio imageio-ffmpeg scipy > /dev/null\n", "print('Dependencies installed.')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Imports & helpers"]}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": ["from __future__ import annotations\n", "import dataclasses, io, math, mmap, os, struct, zipfile, shlex, glob, csv\n", "from typing import Iterator, Optional, Tuple, List\n", "import numpy as np\n", "from PIL import Image\n", "from tqdm import tqdm\n", "import simplekml\n", "\n", "EARTH_R = 6371000.0\n", "\n", "def meters_to_deg_lat(dy_m: float) -> float:\n", "    return (dy_m / EARTH_R) * (180.0 / math.pi)\n", "def meters_to_deg_lon(dx_m: float, lat_deg: float) -> float:\n", "    return (dx_m / (EARTH_R * math.cos(math.radians(lat_deg)))) * (180.0 / math.pi)\n", "def offset_latlon(lat: float, lon: float, dx_east_m: float, dy_north_m: float):\n", "    return lat + meters_to_deg_lat(dy_north_m), lon + meters_to_deg_lon(dx_east_m, lat)\n", "\n", "def haversine_m(lat1, lon1, lat2, lon2):\n", "    if None in (lat1, lon1, lat2, lon2): return None\n", "    r = EARTH_R\n", "    dlat = math.radians(lat2-lat1); dlon = math.radians(lon2-lon1)\n", "    a = math.sin(dlat/2)**2 + math.cos(math.radians(lat1))*math.cos(math.radians(lat2))*math.sin(dlon/2)**2\n", "    return 2*r*math.asin(min(1.0, math.sqrt(a)))\n", "\n", "class _CSV:\n", "    def __init__(self, fp: io.TextIOBase, header: List[str]):\n", "        self.fp = fp; self.fp.write(','.join(header) + '\\n')\n", "    def write_row(self, row: List[str]):\n", "        safe = ['' if s is None else str(s).replace('\"', \"'\").replace(',', ';') for s in row]\n", "        self.fp.write(','.join(safe) + '\\n')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) RSD parser (spec-aligned)"]}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": ["def _read_varuint_from(mv: memoryview, pos: int):\n", "    res = 0; shift = 0\n", "    while True:\n", "        b = mv[pos]; pos += 1\n", "        res |= (b & 0x7F) << shift\n", "        if not (b & 0x80): break\n", "        shift += 7\n", "    return res, pos\n", "def _decode_varuint_bytes(b: bytes) -> int:\n", "    res = 0; shift = 0\n", "    for bb in b:\n", "        res |= (bb & 0x7F) << shift\n", "        if not (bb & 0x80): break\n", "        shift += 7\n", "    return res\n", "def _zigzag_to_int32(u: int) -> int:\n", "    return (u >> 1) ^ (-(u & 1))\n", "\n", "@dataclasses.dataclass\n", "class RSDRecord:\n", "    offset: int\n", "    channel_id: Optional[int]\n", "    sequence_count: int\n", "    data_size: int\n", "    rec_time_ms: int\n", "    lat_deg: Optional[float]\n", "    lon_deg: Optional[float]\n", "    water_temp_c: Optional[float]\n", "    bottom_depth_m: Optional[float]\n", "    sample_cnt: Optional[int]\n", "    sonar_data_offset: Optional[int]\n", "    sonar_data_size: Optional[int]\n", "\n", "class RSDParser:\n", "    MAGIC_REC_HDR = 0xB7E9DA86\n", "    MAGIC_REC_TRL = 0xF98EACBC\n", "    HEADER_AREA_END = 0x5000\n", "    @staticmethod\n", "    def mapunit_to_deg(x: int) -> float:\n", "        return x * (360.0 / float(1 << 32))\n", "    def __init__(self, path: str):\n", "        self.path = path; self.size = os.path.getsize(path)\n", "    def __enter__(self):\n", "        self._f = open(self.path, 'rb'); self.mm = mmap.mmap(self._f.fileno(), 0, access=mmap.ACCESS_READ); return self\n", "    def __exit__(self, exc_type, exc, tb):\n", "        try: self.mm.close()\n", "        finally: self._f.close()\n", "    def _read_varstruct(self, pos: int):\n", "        mv = memoryview(self.mm)\n", "        n_fields, pos = _read_varuint_from(mv, pos)\n", "        fields = []\n", "        for _ in range(n_fields):\n", "            key, pos = _read_varuint_from(mv, pos)\n", "            flen_code = key & 7; field_no = key >> 3\n", "            if flen_code == 7:\n", "                vlen, pos = _read_varuint_from(mv, pos)\n", "            else:\n", "                vlen = flen_code\n", "            val = bytes(mv[pos:pos+vlen]); pos += vlen\n", "            fields.append((field_no, val))\n", "        # trailing CRC (4 bytes) \u2014 ignore value, just advance\n", "        crc = None\n", "        if pos + 4 <= self.size:\n", "            crc = struct.unpack('<I', self.mm[pos:pos+4])[0]\n", "            pos += 4\n", "        return fields, pos, crc\n", "    def parse_records(self) -> Iterator[RSDRecord]:\n", "        pos = self.HEADER_AREA_END\n", "        with tqdm(total=max(0, self.size - pos), unit='B', unit_scale=True, desc='Parse RSD') as pbar:\n", "            while pos + 12 <= self.size:\n", "                rec_start = pos\n", "                try:\n", "                    fields, pos_after_hdr, _ = self._read_varstruct(pos)\n", "                except Exception:\n", "                    break\n", "                magic = None; seq = 0; data_size = 0; rec_time_ms = 0\n", "                for fn, val in fields:\n", "                    if fn == 0 and len(val) == 4: magic = struct.unpack('<I', val)[0]\n", "                    elif fn == 2 and len(val) == 4: seq = struct.unpack('<I', val)[0]\n", "                    elif fn == 4 and len(val) == 2: data_size = struct.unpack('<H', val)[0]\n", "                    elif fn == 5 and len(val) == 4: rec_time_ms = struct.unpack('<I', val)[0]\n", "                if magic != self.MAGIC_REC_HDR:\n", "                    pos = rec_start + 4; pbar.update(4); continue\n", "                pos = pos_after_hdr\n", "                body_start = pos\n", "                channel_id = None; lat_deg = None; lon_deg = None\n", "                water_temp_c = None; bottom_depth_m = None; sample_cnt = None\n", "                sonar_data_offset = None; sonar_data_size = None\n", "                if data_size > 0:\n", "                    b_fields, body_end, _ = self._read_varstruct(body_start)\n", "                    for fn, val in b_fields:\n", "                        if fn == 0: channel_id = int(_decode_varuint_bytes(val))\n", "                        elif fn == 1: bottom_depth_m = _zigzag_to_int32(_decode_varuint_bytes(val)) / 1000.0\n", "                        elif fn == 7 and len(val) == 4: sample_cnt = struct.unpack('<I', val)[0]\n", "                        elif fn == 9 and len(val) == 4: lat_deg = self.mapunit_to_deg(struct.unpack('<i', val)[0])\n", "                        elif fn == 10 and len(val) == 4: lon_deg = self.mapunit_to_deg(struct.unpack('<i', val)[0])\n", "                        elif fn == 11 and len(val) == 4: water_temp_c = struct.unpack('<f', val)[0]\n", "                    used = body_end - body_start\n", "                    sonar_data_offset = body_end\n", "                    sonar_data_size = max(0, data_size - used)\n", "                    pos = body_start + data_size\n", "                else:\n", "                    pos = body_start\n", "                if pos + 12 > self.size: break\n", "                tr_magic, chunk_size, _ = struct.unpack('<III', self.mm[pos:pos+12])\n", "                if tr_magic != self.MAGIC_REC_TRL or chunk_size <= 0:\n", "                    pos = rec_start + 4; pbar.update(4); continue\n", "                yield RSDRecord(\n", "                    offset=rec_start, channel_id=channel_id, sequence_count=seq,\n", "                    data_size=data_size, rec_time_ms=rec_time_ms,\n", "                    lat_deg=lat_deg, lon_deg=lon_deg,\n", "                    water_temp_c=water_temp_c, bottom_depth_m=bottom_depth_m,\n", "                    sample_cnt=sample_cnt, sonar_data_offset=sonar_data_offset,\n", "                    sonar_data_size=sonar_data_size,\n", "                )\n", "                pos = rec_start + chunk_size; pbar.update(max(0, chunk_size))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Configuration + optional synthetic sample `.RSD`"]}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": ["# Rerun from a cached OUT_DIR (recolor & rebuild only):\n", "RERUN_FROM_CACHE = False\n", "CACHE_RUN_DIR = ''  # set to a previous run dir\n", "\n", "# Input RSD (ignored if USE_SAMPLE=True or RERUN_FROM_CACHE=True)\n", "RSD_PATH = ''\n", "\n", "# Orientation\n", "SWAP_SIDES = False\n", "FLIP_PORT  = True\n", "FLIP_STBD  = False\n", "\n", "# Rendering / tiling\n", "ROW_HEIGHT_PX = 40\n", "WATER_COLUMN_PX = 8\n", "SWATH_WIDTH_M = 40.0\n", "MERGE_ROWS = 4\n", "AUTO_DECIMATE = True\n", "TILE_STRIDE = 2\n", "\n", "# KML modes: SINGLE, BUCKETED, REGIONATED\n", "KML_MODE = 'BUCKETED'\n", "BUCKET_DEG = 0.01\n", "MIN_LOD_PX = 64\n", "MAX_LOD_PX = -1\n", "\n", "# Tone & palettes\n", "INVERT_INTENSITY = True\n", "CLIP_LOW_PCT  = 1.0\n", "CLIP_HIGH_PCT = 99.0\n", "GAMMA = 1.0\n", "MULTI_PALETTES = 'amber,blue,grayscale'\n", "\n", "# Preview & video\n", "MAKE_PREVIEW = True\n", "PREVIEW_MAX_ROWS = 2000\n", "MAKE_VIDEO = False\n", "VIDEO_FPS = 15\n", "VIDEO_MAX_WIDTH = 1280\n", "\n", "# Extras\n", "MAKE_GPS_MARKERS = True\n", "GPS_MARKER_INTERVAL_M = 250\n", "MAKE_DEPTH_OVERLAY = True\n", "MAKE_TARGETS = True\n", "TARGET_THRESHOLD = 220\n", "TARGET_MIN_PIXELS = 30\n", "TARGET_PING_GAP = 3\n", "MAKE_ZIP = True\n", "ZIP_INCLUDE_STRIPS = False\n", "\n", "# Use a tiny synthetic .RSD for demo / GitHub CI\n", "USE_SAMPLE = True\n", "\n", "def _write_synthetic_rsd(path: str, n_pings=12, base_lat=43.132, base_lon=-83.432):\n", "    # Minimal fake RSD-like structure to exercise the pipeline:\n", "    # header area (0x5000), records with header/body/trailer, sonar bytes inside body.\n", "    with open(path, 'wb') as f:\n", "        f.write(b'\\x00'*0x5000)  # header area\n", "        seq = 0\n", "        for i in range(n_pings):\n", "            # ---- header varstruct ----\n", "            hdr = bytearray()\n", "            hdr += b'\\x04'  # n_fields=4\n", "            hdr += b'\\x04' + struct.pack('<I', 0xB7E9DA86)        # field 0, magic\n", "            hdr += b'\\x14' + struct.pack('<I', seq)               # field 2, seq\n", "            hdr += b'\\x22' + b'\\x00\\x00'                         # field 4, data_size (patch later)\n", "            hdr += b'\\x2C' + struct.pack('<I', 1000*i)            # field 5, time_ms\n", "            hdr_crc = struct.pack('<I', 0)\n", "\n", "            # ---- body varstruct ----\n", "            body = bytearray()\n", "            body += b'\\x07'  # n_fields=7\n", "            body += b'\\x00' + b'\\x02'                             # ch id = 2\n", "            depth_mm = 5000 + 50*i\n", "            zz = (depth_mm<<1) ^ (depth_mm>>31)\n", "            body += b'\\x08' + bytes([zz & 0x7F])                  # depth_mm zigzag (tiny)\n", "            body += b'\\x38' + struct.pack('<I', 512)              # sample_cnt=512\n", "            lat_mu = int((base_lat + 0.0005*i) * (1<<32) / 360.0)\n", "            lon_mu = int((base_lon + 0.0005*i) * (1<<32) / 360.0)\n", "            body += b'\\x48' + struct.pack('<i', lat_mu)           # lat\n", "            body += b'\\x50' + struct.pack('<i', lon_mu)           # lon\n", "            body += b'\\x58' + struct.pack('<f', 18.0 + 0.1*i)     # temp\n", "            body_crc = struct.pack('<I', 0)\n", "\n", "            # sonar bytes (u8, 2ch) inside body region\n", "            sonar = os.urandom(1024)\n", "            data_size = len(body) + 4 + len(sonar)\n", "            hdr_patched = bytearray(hdr)\n", "            # Patch data_size value (find the two bytes after field tag 0x22)\n", "            # hdr layout: n(1) + [f0 tag(1)+4] + [f2 tag(1)+4] + [f4 tag(1)+2] + [f5 tag(1)+4]\n", "            # indexes:          0..5             6..11            12..14             15..20\n", "            hdr_patched[13:15] = struct.pack('<H', data_size)\n", "\n", "            # write: header, hdr_crc, body, body_crc, sonar, then trailer\n", "            f.write(hdr_patched); f.write(hdr_crc)\n", "            body_start_len = len(body) + 4\n", "            f.write(body); f.write(body_crc)\n", "            f.write(sonar)\n", "\n", "            # trailer (covers header+hdr_crc + body+body_crc + sonar + trailer itself)\n", "            chunk_size = len(hdr_patched)+4 + body_start_len + len(sonar) + 12\n", "            f.write(struct.pack('<I', 0xF98EACBC))  # magic\n", "            f.write(struct.pack('<I', chunk_size))\n", "            f.write(struct.pack('<I', 0))           # crc\n", "            seq += 1\n", "\n", "if USE_SAMPLE:\n", "    SAMPLE_PATH = '/content/sample.rsd'\n", "    _write_synthetic_rsd(SAMPLE_PATH)\n", "    RSD_PATH = SAMPLE_PATH\n", "    print('Synthetic sample written:', RSD_PATH)\n", "else:\n", "    print('Upload your .RSD via the file browser or set RSD_PATH manually.')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) Tone mapping & palettes"]}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": ["def make_palette(name: str) -> np.ndarray:\n", "    name = (name or 'grayscale').lower().strip()\n", "    x = np.linspace(0, 1, 256)\n", "    if name == 'amber':\n", "        r = np.clip(3.0*x, 0, 1); g = np.clip(1.8*x, 0, 1); b = np.clip(0.5*x, 0, 1)\n", "    elif name == 'blue':\n", "        r = np.clip(0.4*x, 0, 1); g = np.clip(0.7*x, 0, 1); b = np.clip(1.8*x, 0, 1)\n", "    elif name == 'green':\n", "        r = np.clip(0.5*x, 0, 1); g = np.clip(1.8*x, 0, 1); b = np.clip(0.5*x, 0, 1)\n", "    elif name == 'ironbow':\n", "        r = np.clip(1.5*x, 0, 1); g = np.clip(1.5*np.maximum(x-0.33,0), 0, 1); b = np.clip(1.5*np.maximum(x-0.66,0), 0, 1)\n", "    else:\n", "        r = g = b = x\n", "    return (np.stack([r,g,b], axis=1) * 255.0 + 0.5).astype(np.uint8)\n", "\n", "def tone_map(u8: np.ndarray) -> np.ndarray:\n", "    a = u8.astype(np.float32)\n", "    if INVERT_INTENSITY: a = 255.0 - a\n", "    lo = np.percentile(a, CLIP_LOW_PCT) if 0 <= CLIP_LOW_PCT < 50 else a.min()\n", "    hi = np.percentile(a, CLIP_HIGH_PCT) if 50 < CLIP_HIGH_PCT <= 100 else a.max()\n", "    if hi <= lo: hi = lo + 1.0\n", "    a = (a - lo) / (hi - lo)\n", "    if GAMMA != 1.0: a = np.power(np.clip(a, 0, 1), GAMMA)\n", "    return np.clip(a * 255.0, 0, 255).astype(np.uint8)\n", "\n", "def _normalize_to_u8(arr: np.ndarray, is_u16: bool) -> np.ndarray:\n", "    a = arr.astype(np.float32)\n", "    if is_u16: a *= (255.0/65535.0)\n", "    return np.clip(a, 0, 255).astype(np.uint8)\n", "\n", "def _infer_layout(blob_len: int, sample_cnt: Optional[int]):\n", "    sc = int(sample_cnt or 0)\n", "    if sc > 0:\n", "        if blob_len == sc:     return ('u8', 1)\n", "        if blob_len == 2*sc:   return ('u8', 2)\n", "        if blob_len == 4*sc:   return ('u16', 2)\n", "        if blob_len == 2*sc:   return ('u16', 1)\n", "        ratio = blob_len / sc\n", "        if abs(ratio-2.0) < 0.1: return ('u8', 2)\n", "        if abs(ratio-1.0) < 0.1: return ('u8', 1)\n", "        if abs(ratio-4.0) < 0.2: return ('u16', 2)\n", "        if abs(ratio-2.0) < 0.2: return ('u16', 1)\n", "    return ('u8', 2)\n", "\n", "def assemble_scan(a: np.ndarray, sc: int, chans: int) -> np.ndarray:\n", "    if sc <= 0: sc = a.size // chans\n", "    if chans == 2 and a.size >= 2*sc:\n", "        port = a[:sc]; stbd = a[sc:2*sc]\n", "        if FLIP_PORT: port = port[::-1]\n", "        if FLIP_STBD: stbd = stbd[::-1]\n", "        if SWAP_SIDES: port, stbd = stbd, port\n", "        scan = np.hstack([port, np.zeros(WATER_COLUMN_PX, dtype=np.uint8), stbd])\n", "    else:\n", "        scan = a[:sc]\n", "    return scan\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7) Parse \u2192 CSV/track \u2192 grayscale row cache"]}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": ["def load_or_parse_rsd(path, out_dir):\n", "    base = os.path.splitext(os.path.basename(path))[0]\n", "    recs = []\n", "    with RSDParser(path) as P:\n", "        for rec in P.parse_records(): recs.append(rec)\n", "    # CSV\n", "    csv_path = os.path.join(out_dir, f\"{base}_records.csv\")\n", "    with open(csv_path, 'w', encoding='utf-8') as fp:\n", "        w = _CSV(fp, ['file_ofs','channel_id','seq','time_ms','lat_deg','lon_deg','water_temp_c','bottom_depth_m','sample_cnt','sonar_ofs','sonar_size'])\n", "        for r in recs:\n", "            w.write_row([r.offset, r.channel_id or '', r.sequence_count, r.rec_time_ms,\n", "                         f\"{r.lat_deg:.8f}\" if r.lat_deg is not None else '',\n", "                         f\"{r.lon_deg:.8f}\" if r.lon_deg is not None else '',\n", "                         f\"{r.water_temp_c:.2f}\" if r.water_temp_c is not None else '',\n", "                         f\"{r.bottom_depth_m:.3f}\" if r.bottom_depth_m is not None else '',\n", "                         r.sample_cnt or '', r.sonar_data_offset or '', r.sonar_data_size or ''])\n", "    # Track\n", "    trk_kml = os.path.join(out_dir, f\"{base}_track.kml\")\n", "    k = simplekml.Kml(); ls = k.newlinestring(name=f\"{base} track\"); coords=[]\n", "    for r in recs:\n", "        if r.lat_deg is None or r.lon_deg is None: continue\n", "        alt = -float(r.bottom_depth_m) if r.bottom_depth_m is not None else 0.0\n", "        coords.append((r.lon_deg, r.lat_deg, alt))\n", "    ls.coords = coords; k.save(trk_kml)\n", "\n", "    # Grayscale rows cache\n", "    with open(path, 'rb') as f:\n", "        preview_rows = []\n", "        for idx, r in enumerate(recs):\n", "            if not r.sonar_data_offset or not r.sonar_data_size or r.sonar_data_size <= 0: continue\n", "            f.seek(r.sonar_data_offset); blob = f.read(r.sonar_data_size)\n", "            dtype, chans = _infer_layout(len(blob), r.sample_cnt)\n", "            if dtype == 'u8': a = np.frombuffer(blob, dtype=np.uint8)\n", "            else: a = np.frombuffer(blob, dtype='<u2'); a = _normalize_to_u8(a, True)\n", "            sc = int(r.sample_cnt or 0)\n", "            gray = tone_map(assemble_scan(a, sc, chans))\n", "            if gray.ndim == 1: gray = gray[np.newaxis, :]\n", "            img = Image.fromarray(gray, 'L').resize((gray.shape[1], ROW_HEIGHT_PX))\n", "            img.save(os.path.join(out_dir, f\"{base}_rowgray_{idx:06d}.png\"))\n", "            if MAKE_PREVIEW and idx < PREVIEW_MAX_ROWS:\n", "                preview_rows.append(np.array(img.convert('RGB')))\n", "            if (idx+1) % 2000 == 0: print('rows (gray) written:', idx+1)\n", "    if MAKE_PREVIEW and preview_rows:\n", "        prev = np.vstack(preview_rows)\n", "        Image.fromarray(prev, 'RGB').save(os.path.join(out_dir, f\"{base}_grayscale_waterfall.png\"))\n", "    return base, recs, trk_kml\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8) Recolor rows & build tiles (with padding fix)"]}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": ["def recolor_rows(out_dir, base, palette_name, make_preview=True):\n", "    lut = make_palette(palette_name)\n", "    row_files = sorted(glob.glob(os.path.join(out_dir, f\"{base}_rowgray_*.png\")))\n", "    preview_rows = []\n", "    for p in row_files:\n", "        gray = np.array(Image.open(p))\n", "        rgb = lut[gray]\n", "        op = p.replace('_rowgray_', f'_{palette_name}_row_')\n", "        Image.fromarray(rgb, 'RGB').save(op)\n", "        if make_preview and len(preview_rows) < PREVIEW_MAX_ROWS:\n", "            preview_rows.append(rgb)\n", "    if make_preview and preview_rows:\n", "        prev = np.vstack(preview_rows)\n", "        Image.fromarray(prev, 'RGB').save(os.path.join(out_dir, f\"{base}_{palette_name}_waterfall.png\"))\n", "\n", "def segment_quad(latA, lonA, latB, lonB, half_width_m):\n", "    dN = (latB - latA) * (math.pi/180.0) * EARTH_R\n", "    dE = (lonB - lonA) * (math.pi/180.0) * EARTH_R * math.cos(math.radians((latA+latB)/2.0))\n", "    L  = max(1e-6, math.hypot(dE, dN))\n", "    uE, uN = dE/L, dN/L\n", "    pE, pN = -uN, uE\n", "    A_left  = offset_latlon(latA, lonA,  pE*half_width_m,  pN*half_width_m)\n", "    A_right = offset_latlon(latA, lonA, -pE*half_width_m, -pN*half_width_m)\n", "    B_left  = offset_latlon(latB, lonB,  pE*half_width_m,  pN*half_width_m)\n", "    B_right = offset_latlon(latB, lonB, -pE*half_width_m, -pN*half_width_m)\n", "    return [(A_left[1],A_left[0]), (A_right[1],A_right[0]), (B_right[1],B_right[0]), (B_left[1],B_left[0])]\n", "\n", "def _pad_row_center_to_width(row_rgb: np.ndarray, target_w: int) -> np.ndarray:\n", "    h, w, c = row_rgb.shape\n", "    if w == target_w:\n", "        return row_rgb\n", "    delta = target_w - w\n", "    left = delta // 2\n", "    right = delta - left\n", "    return np.pad(row_rgb, ((0,0), (left,right), (0,0)), mode='constant', constant_values=0)\n", "\n", "def build_tiles(out_dir, base, recs, palette_name, merge_rows, tile_stride, swath_m):\n", "    used = 0; i = 0; tiles_meta = []\n", "    while i < len(recs) - 1:\n", "        valid_pairs = []; rows_for_tile = []\n", "        j = i\n", "        while j < min(i + merge_rows, len(recs) - 1):\n", "            rA, rB = recs[j], recs[j+1]\n", "            if None not in (rA.lat_deg, rA.lon_deg, rB.lat_deg, rB.lon_deg):\n", "                fp = os.path.join(out_dir, f\"{base}_{palette_name}_row_{j:06d}.png\")\n", "                if os.path.exists(fp):\n", "                    rows_for_tile.append(np.array(Image.open(fp)))\n", "                    valid_pairs.append((rA, rB))\n", "            j += 1\n", "        if rows_for_tile and valid_pairs:\n", "            max_w = max(r.shape[1] for r in rows_for_tile)\n", "            rows_for_tile = [_pad_row_center_to_width(r, max_w) for r in rows_for_tile]\n", "            tile_img = np.vstack(rows_for_tile)\n", "            tname = f\"{base}_{palette_name}_tile_{i:06d}.png\"\n", "            tpath = os.path.join(out_dir, tname)\n", "            Image.fromarray(tile_img, 'RGB').save(tpath)\n", "            rA0, rB1 = valid_pairs[0][0], valid_pairs[-1][1]\n", "            quad = segment_quad(rA0.lat_deg, rA0.lon_deg, rB1.lat_deg, rB1.lon_deg, swath_m)\n", "            lons = [q[0] for q in quad]; lats = [q[1] for q in quad]\n", "            bbox = dict(n=max(lats), s=min(lats), e=max(lons), w=min(lons))\n", "            tiles_meta.append(dict(name=tname, path=tpath, quad=quad, bbox=bbox))\n", "            used += 1\n", "        i += max(1, tile_stride)\n", "    return tiles_meta, used\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9) KML writers (SINGLE, BUCKETED, REGIONATED)"]}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": ["def kml_region(bbox, min_px, max_px):\n", "    return f\"\"\"\n", "    <Region>\n", "      <LatLonAltBox>\n", "        <north>{bbox['n']}</north>\n", "        <south>{bbox['s']}</south>\n", "        <east>{bbox['e']}</east>\n", "        <west>{bbox['w']}</west>\n", "      </LatLonAltBox>\n", "      <Lod>\n", "        <minLodPixels>{int(min_px)}</minLodPixels>\n", "        <maxLodPixels>{int(max_px)}</maxLodPixels>\n", "      </Lod>\n", "    </Region>\n", "    \"\"\".strip()\n", "\n", "def write_kmz_single(out_dir, base, palette_name, tiles_meta):\n", "    kmz_path = os.path.join(out_dir, f\"{base}_{palette_name}_sidescan.kmz\")\n", "    images_dir_in_kmz = 'files'\n", "    kml = ['<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n", "           '<kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\">',\n", "           '<Document>', f'<name>{base} {palette_name} sidescan</name>']\n", "    for T in tiles_meta:\n", "        coords_txt = ' '.join([f\"{lon:.8f},{lat:.8f}\" for lon,lat in T['quad']])\n", "        kml += ['<GroundOverlay>', f'  <name>{T[\"name\"]}</name>', '  <Icon>',\n", "                f'    <href>files/' + T['name'] + '</href>', '  </Icon>',\n", "                '  <gx:LatLonQuad>', f'    <coordinates>{coords_txt}</coordinates>', '  </gx:LatLonQuad>',\n", "                '</GroundOverlay>']\n", "    kml += ['</Document>', '</kml>']\n", "    with zipfile.ZipFile(kmz_path, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n", "        zf.writestr('doc.kml', '\\n'.join(kml))\n", "        for T in tiles_meta: zf.write(T['path'], f\"{images_dir_in_kmz}/{T['name']}\")\n", "    return kmz_path\n", "\n", "def assign_buckets(tiles_meta, bucket_deg):\n", "    buckets = {}\n", "    for T in tiles_meta:\n", "        latc = sum([p[1] for p in T['quad']])/4.0\n", "        lonc = sum([p[0] for p in T['quad']])/4.0\n", "        ib = int(math.floor(latc / bucket_deg))\n", "        jb = int(math.floor(lonc / bucket_deg))\n", "        buckets.setdefault((ib,jb), []).append(T)\n", "    return buckets\n", "\n", "def write_kmz_bucketed(out_dir, base, palette_name, tiles_meta, bucket_deg):\n", "    kmz_path = os.path.join(out_dir, f\"{base}_{palette_name}_sidescan.kmz\")\n", "    images_dir_in_kmz = 'files'\n", "    buckets = assign_buckets(tiles_meta, bucket_deg)\n", "    master = ['<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n", "              '<kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\">',\n", "              '<Document>', f'<name>{base} {palette_name} sidescan (bucketed)</name>']\n", "    for (ib,jb), tiles in buckets.items():\n", "        master += [f'<Folder><name>bucket {ib},{jb}</name>']\n", "        for T in tiles:\n", "            coords_txt = ' '.join([f\"{lon:.8f},{lat:.8f}\" for lon,lat in T['quad']])\n", "            master += ['<GroundOverlay>', f'  <name>{T[\"name\"]}</name>', '  <Icon>',\n", "                       f'    <href>files/' + T['name'] + '</href>', '  </Icon>',\n", "                       '  <gx:LatLonQuad>', f'    <coordinates>{coords_txt}</coordinates>', '  </gx:LatLonQuad>',\n", "                       '</GroundOverlay>']\n", "        master += ['</Folder>']\n", "    master += ['</Document>', '</kml>']\n", "    with zipfile.ZipFile(kmz_path, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n", "        zf.writestr('doc.kml', '\\n'.join(master))\n", "        for T in tiles_meta: zf.write(T['path'], f\"{images_dir_in_kmz}/{T['name']}\")\n", "    return kmz_path\n", "\n", "def write_kmz_regionated(out_dir, base, palette_name, tiles_meta, bucket_deg, min_px, max_px):\n", "    kmz_path = os.path.join(out_dir, f\"{base}_{palette_name}_sidescan.kmz\")\n", "    images_dir_in_kmz = 'files'\n", "    buckets = assign_buckets(tiles_meta, bucket_deg)\n", "    def bucket_bbox(tiles):\n", "        n = max(t['bbox']['n'] for t in tiles)\n", "        s = min(t['bbox']['s'] for t in tiles)\n", "        e = max(t['bbox']['e'] for t in tiles)\n", "        w = min(t['bbox']['w'] for t in tiles)\n", "        return dict(n=n,s=s,e=e,w=w)\n", "    master = ['<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n", "              '<kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\">',\n", "              '<Document>', f'<name>{base} {palette_name} sidescan (regionated)</name>']\n", "    child_docs = []\n", "    for key, tiles in buckets.items():\n", "        bbox = bucket_bbox(tiles)\n", "        child_name = f\"bucket_{key[0]}_{key[1]}.kml\"\n", "        master += ['<NetworkLink>', f'  <name>{child_name}</name>', kml_region(bbox, min_px, max_px),\n", "                   '  <Link>', f'    <href>kml/' + child_name + '</href>', '    <viewRefreshMode>onRegion</viewRefreshMode>', '  </Link>', '</NetworkLink>']\n", "        child = ['<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n", "                 '<kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\">',\n", "                 '<Document>', f'<name>{child_name}</name>']\n", "        for T in tiles:\n", "            coords_txt = ' '.join([f\"{lon:.8f},{lat:.8f}\" for lon,lat in T['quad']])\n", "            child += ['<GroundOverlay>', f'  <name>{T[\"name\"]}</name>', kml_region(T['bbox'], min_px, max_px),\n", "                      '  <Icon>', f'    <href>../files/' + T['name'] + '</href>', '  </Icon>',\n", "                      '  <gx:LatLonQuad>', f'    <coordinates>{coords_txt}</coordinates>', '  </gx:LatLonQuad>', '</GroundOverlay>']\n", "        child += ['</Document>', '</kml>']\n", "        child_docs.append((child_name, '\\n'.join(child)))\n", "    master += ['</Document>', '</kml>']\n", "    with zipfile.ZipFile(kmz_path, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n", "        zf.writestr('doc.kml', '\\n'.join(master))\n", "        for name, text in child_docs: zf.writestr(f'kml/{name}', text)\n", "        for T in tiles_meta: zf.write(T['path'], f\"{images_dir_in_kmz}/{T['name']}\")\n", "    return kmz_path\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 10) Waterfall video export (ffmpeg)"]}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": ["def ffmpeg_video_from_rows(out_dir, base, palette_name, fps=15, max_w=1280):\n", "    list_path = os.path.join(out_dir, f\"{base}_{palette_name}_frames.txt\")\n", "    with open(list_path, 'w') as f:\n", "        for p in sorted(glob.glob(os.path.join(out_dir, f\"{base}_{palette_name}_row_*.png\"))):\n", "            f.write(f\"file '{p}'\\n\")\n", "    mp4_path = os.path.join(out_dir, f\"{base}_{palette_name}_waterfall.mp4\")\n", "    cmd = f\"ffmpeg -y -f concat -safe 0 -i {shlex.quote(list_path)} -vf \\\"scale=min(iw\\\\,{int(max_w)}):-2,fps={int(fps)}\\\" -c:v libx264 -pix_fmt yuv420p -preset veryfast -crf 23 {shlex.quote(mp4_path)}\"\n", "    print(cmd); os.system(cmd); print('Video:', mp4_path); return mp4_path\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 11) GPS markers (sanity checks)"]}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": ["def gps_markers_kml(out_dir, base, recs, interval_m):\n", "    k = simplekml.Kml(); last_lat=None; last_lon=None; acc=0.0; count=0\n", "    for r in recs:\n", "        if r.lat_deg is None or r.lon_deg is None: continue\n", "        if last_lat is None:\n", "            last_lat, last_lon = r.lat_deg, r.lon_deg\n", "            k.newpoint(name='Start', coords=[(r.lon_deg, r.lat_deg)])\n", "            continue\n", "        d = haversine_m(last_lat,last_lon,r.lat_deg,r.lon_deg) or 0.0\n", "        acc += d; last_lat, last_lon = r.lat_deg, r.lon_deg\n", "        if acc >= interval_m:\n", "            count += 1; acc = 0.0\n", "            k.newpoint(name=f'GP {count}', coords=[(r.lon_deg, r.lat_deg)])\n", "    path = os.path.join(out_dir, f\"{base}_gpsmarkers.kml\"); k.save(path); return path\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 12) Depth overlay (CSV + KML path)"]}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": ["def depth_overlay(out_dir, base, recs):\n", "    csv_p = os.path.join(out_dir, f\"{base}_depth.csv\")\n", "    with open(csv_p, 'w', newline='', encoding='utf-8') as fp:\n", "        w = csv.writer(fp); w.writerow(['time_ms','lat_deg','lon_deg','depth_m','temp_c'])\n", "        for r in recs:\n", "            w.writerow([r.rec_time_ms, r.lat_deg, r.lon_deg, r.bottom_depth_m, r.water_temp_c])\n", "    k = simplekml.Kml(); ls = k.newlinestring(name=f\"{base} depth\"); coords=[]\n", "    for r in recs:\n", "        if r.lat_deg is None or r.lon_deg is None: continue\n", "        coords.append((r.lon_deg, r.lat_deg))\n", "    ls.coords = coords; ls.extrude = 0; ls.altitudemode = simplekml.AltitudeMode.clamptoground\n", "    kml_p = os.path.join(out_dir, f\"{base}_depth.kml\"); k.save(kml_p)\n", "    return csv_p, kml_p\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 13) Target detection (simple bright-blob heuristic)"]}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": ["def detect_targets(out_dir, base, recs, thresh=220, min_pixels=30, ping_gap=3):\n", "    from scipy import ndimage as ndi\n", "    rows = sorted(glob.glob(os.path.join(out_dir, f\"{base}_rowgray_*.png\")))\n", "    targets = []\n", "    for p in rows:\n", "        idx = int(os.path.basename(p).split('_rowgray_')[1].split('.')[0])\n", "        rA = recs[idx] if idx < len(recs) else None\n", "        if rA is None or rA.lat_deg is None or rA.lon_deg is None: continue\n", "        img = np.array(Image.open(p))\n", "        mask = img >= int(thresh)\n", "        if not mask.any(): continue\n", "        lab, n = ndi.label(mask)\n", "        if n == 0: continue\n", "        areas = ndi.sum(mask, lab, index=np.arange(1,n+1))\n", "        means = ndi.mean(img, lab, index=np.arange(1,n+1))\n", "        for k_lbl in range(1, n+1):\n", "            area = int(areas[k_lbl-1])\n", "            if area < min_pixels: continue\n", "            meanv = float(means[k_lbl-1])\n", "            targets.append((idx, rA.lat_deg, rA.lon_deg, area, meanv))\n", "    targets.sort(key=lambda x: x[0])\n", "    merged = []\n", "    for t in targets:\n", "        if not merged or t[0] - merged[-1][0] > ping_gap:\n", "            merged.append(list(t))\n", "        else:\n", "            if t[3]*t[4] > merged[-1][3]*merged[-1][4]: merged[-1] = list(t)\n", "    csv_p = os.path.join(out_dir, f\"{base}_targets.csv\")\n", "    with open(csv_p, 'w', newline='', encoding='utf-8') as fp:\n", "        w = csv.writer(fp); w.writerow(['ping_idx','lat','lon','area_px','mean_u8'])\n", "        for t in merged: w.writerow(t)\n", "    k = simplekml.Kml()\n", "    for i, t in enumerate(merged, 1):\n", "        k.newpoint(name=f'Target {i}', coords=[(t[2], t[1])], description=f'ping={t[0]}, area_px={t[3]}, mean={t[4]:.1f}')\n", "    kml_p = os.path.join(out_dir, f\"{base}_targets.kml\"); k.save(kml_p)\n", "    return csv_p, kml_p, len(merged)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 14) Orchestrate pipeline + ZIP export"]}, {"cell_type": "code", "metadata": {}, "execution_count": 0, "outputs": [], "source": ["def auto_stride(n_pings:int) -> int:\n", "    if n_pings <= 20_000: return 1\n", "    if n_pings <= 50_000: return 2\n", "    if n_pings <= 100_000: return 3\n", "    if n_pings <= 200_000: return 5\n", "    return 8\n", "\n", "if RERUN_FROM_CACHE:\n", "    assert os.path.isdir(CACHE_RUN_DIR), 'Set CACHE_RUN_DIR to a prior run folder.'\n", "    OUT_DIR = CACHE_RUN_DIR\n", "    gray_files = sorted(glob.glob(os.path.join(OUT_DIR, '*_rowgray_*.png')))\n", "    assert gray_files, 'No cached grayscale rows found.'\n", "    base = os.path.basename(gray_files[0]).split('_rowgray_')[0]\n", "    rec_csv = os.path.join(OUT_DIR, f\"{base}_records.csv\")\n", "    recs = []\n", "    with open(rec_csv, 'r', encoding='utf-8') as fp:\n", "        hdr = fp.readline()\n", "        for line in fp:\n", "            cols = line.strip().split(',')\n", "            def f(x):\n", "                try: return float(x) if x else None\n", "                except: return None\n", "            recs.append(RSDRecord(0,None,0,0,0,f(cols[4]),f(cols[5]),None,f(cols[7]),None,None,None))\n", "else:\n", "    assert RSD_PATH and os.path.exists(RSD_PATH), 'RSD_PATH not set or file not found.'\n", "    base, recs, trk = load_or_parse_rsd(RSD_PATH, OUT_DIR)\n", "\n", "n_pings = len(recs)\n", "stride = (auto_stride(n_pings) if AUTO_DECIMATE else max(1, int(TILE_STRIDE)))\n", "print(f'Ping count: {n_pings} -> TILE_STRIDE={stride}')\n", "\n", "palettes = [p.strip() for p in MULTI_PALETTES.split(',') if p.strip()]\n", "kmz_paths = []\n", "for pal in palettes:\n", "    print('Recolor:', pal)\n", "    recolor_rows(OUT_DIR, base, pal, make_preview=MAKE_PREVIEW)\n", "    tiles_meta, used = build_tiles(OUT_DIR, base, recs, pal, MERGE_ROWS, stride, SWATH_WIDTH_M)\n", "    print(f'Tiles built ({pal}):', used)\n", "    if KML_MODE == 'SINGLE':\n", "        kmz = write_kmz_single(OUT_DIR, base, pal, tiles_meta)\n", "    elif KML_MODE == 'BUCKETED':\n", "        kmz = write_kmz_bucketed(OUT_DIR, base, pal, tiles_meta, BUCKET_DEG)\n", "    else:\n", "        kmz = write_kmz_regionated(OUT_DIR, base, pal, tiles_meta, BUCKET_DEG, MIN_LOD_PX, MAX_LOD_PX)\n", "    kmz_paths.append(kmz)\n", "    if MAKE_VIDEO:\n", "        ffmpeg_video_from_rows(OUT_DIR, base, pal, fps=VIDEO_FPS, max_w=VIDEO_MAX_WIDTH)\n", "\n", "if MAKE_GPS_MARKERS:\n", "    gp = gps_markers_kml(OUT_DIR, base, recs, GPS_MARKER_INTERVAL_M)\n", "    print('GPS markers KML:', gp)\n", "if MAKE_DEPTH_OVERLAY:\n", "    dcsv, dkml = depth_overlay(OUT_DIR, base, recs)\n", "    print('Depth CSV:', dcsv); print('Depth KML:', dkml)\n", "if MAKE_TARGETS:\n", "    tcsv, tkml, nT = detect_targets(OUT_DIR, base, recs, TARGET_THRESHOLD, TARGET_MIN_PIXELS, TARGET_PING_GAP)\n", "    print(f'Targets: {nT} -> {tcsv}, {tkml}')\n", "\n", "if MAKE_ZIP:\n", "    zip_name = os.path.basename(OUT_DIR.rstrip('/')) + '.zip'\n", "    zip_path = os.path.join(OUT_DIR, zip_name)\n", "    with zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_DEFLATED) as z:\n", "        pats = [f\"*records.csv\", f\"*track.kml\", f\"*targets.*\", f\"*depth.*\", f\"*_*_sidescan.kmz\", f\"*_*_waterfall.png\", f\"*_*_waterfall.mp4\"]\n", "        for pat in pats:\n", "            for p in glob.glob(os.path.join(OUT_DIR, pat)):\n", "                z.write(p, os.path.basename(p))\n", "        if ZIP_INCLUDE_STRIPS:\n", "            for p in glob.glob(os.path.join(OUT_DIR, f\"*_*_row_*.png\")):\n", "                z.write(p, os.path.join('strips', os.path.basename(p)))\n", "    print('ZIP:', zip_path)\n", "    try:\n", "        from google.colab import files as _files\n", "        _files.download(zip_path)\n", "    except Exception:\n", "        pass\n", "\n", "print('Done. OUT_DIR:', OUT_DIR)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.12"}}, "nbformat": 4, "nbformat_minor": 2}